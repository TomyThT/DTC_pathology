{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef1d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#代码源自https://blog.csdn.net/AI_dataloads/article/details/134126532\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "from histolab.slide import Slide\n",
    "import csv\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, confusion_matrix, make_scorer, auc\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from itertools import cycle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80216be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_initialization(dimension=4,freeze=True):\n",
    "    resnet_model = models.resnet152(weights=models.ResNet152_Weights.DEFAULT)\n",
    "    for param in resnet_model.parameters():\n",
    "        param.requires_grad = not freeze\n",
    "    in_features = resnet_model.fc.in_features\n",
    "    resnet_model.fc = nn.Linear(in_features,dimension) #全连接层改为输出4维向量\n",
    "    params_to_update = []\n",
    "    for param in resnet_model.parameters():\n",
    "        if param.requires_grad:\n",
    "            params_to_update.append(param)\n",
    "    return resnet_model,params_to_update\n",
    "def model_load(resnet_model,device,model_pth):\n",
    "    # 加载保存的模型权重\n",
    "    state_dict = torch.load(model_pth,weights_only = True)#\"stage1/model_stage1_epochs_1.pth\")  # 替换为你的文件路径\n",
    "    resnet_model.load_state_dict(state_dict)\n",
    "    # 将模型移动到设备（CPU 或 GPU）\n",
    "    resnet_model.to(device)\n",
    "    # 切换到评估模式\n",
    "    resnet_model.eval()\n",
    "    return resnet_model\n",
    "\n",
    "#resnet_model,params_to_update = model_initialization(4)\n",
    "#print(params_to_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67976e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = { #也可以使用PIL库，smote 人工拟合出来数据\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize([256,256]),#是图像变換大小\n",
    "        transforms.RandomRotation(45),#随机旋转，-45 到45度之间随机选择\n",
    "        #transforms.CenterCrop(256),#从中心开始裁剪[256, 256]\n",
    "        transforms.RandomHorizontalFlip(p=0.5),#随机水平翻转 选择一个概率概率\n",
    "        transforms.RandomVerticalFlip(p=0.5),#随机垂直翻转\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.1, saturation=0.1, hue=0.1),#参数1为亮度，参数2为对比度,参数3为饱和度,参数4为色相\n",
    "        transforms.Lambda(lambda x: x.convert('RGB')), #将图像转为RGB 3通道\n",
    "        transforms.RandomGrayscale(p=0.1),#概率转换成灰度率，3通道就是R=G=B\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])#标准化，均值，标准差\n",
    "    ]),\n",
    "    'valid':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize([256,256]),\n",
    "        transforms.Lambda(lambda x: x.convert('RGB')), #将图像转为RGB 3通道\n",
    "        #transforms.Grayscale(3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff55c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#做了数据增强不代表 训练效果一定会变好，只能说大概率上会变好\n",
    "class wsi_dataset(Dataset): #food_dataset是自己创建的类名称，可以改为你需要的名称\n",
    "    def __init__(self, file_path,transform=None):#类的初始化\n",
    "        self.file_path = file_path\n",
    "        self.imgs = []\n",
    "        self.labels = []    \n",
    "        self.transform = transform\n",
    "        with open(self.file_path) as f:\n",
    "            #samples = [x.strip().split(' ') for x in f.readlines()]\n",
    "            samples = [(x[:x.rfind(\" \")].strip('\\\"'), x[x.rfind(\" \") + 1:].strip()) for x in f.readlines()]\n",
    "            for img_path, label in samples:\n",
    "                self.imgs.append(img_path)\n",
    "                self.labels.append(label)\n",
    "    def __len__(self): #类实例化对象后，可以使用Len函数测量对象的个数\n",
    "        return len(self.imgs)\n",
    "        \n",
    "    def __getitem__(self, idx):#关键，可通过索引的形式获取每一个图片数据及标签\n",
    "        image = Image.open(self.imgs[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        label = torch.from_numpy(np.array(label,dtype = np.int64))\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6c7f43",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#获得训练集和验证集\n",
    "data_folder = r\"E:\\THT\\DTC_histology\\cut\"\n",
    "# 使用 glob 递归查找以 \"tile\" 开头的 .png 文件\n",
    "file_pattern = os.path.join(data_folder, \"**\", \"tile*.png\")\n",
    "files = glob.glob(file_pattern, recursive=True)\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa684bf2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#根据不同type分别提取 并输出txt\n",
    "all_txt = []\n",
    "type_list = [\"tumor\",\"stroma\",\"normal\",\"immune\"]\n",
    "#type_sum = [0] * len(type_list)\n",
    "type_files = {t: [] for t in type_list}\n",
    "for file in files:\n",
    "    in_type = False\n",
    "    for i in range(len(type_list)):\n",
    "        if type_list[i] in file:\n",
    "            all_txt.append(f\"{file} {i}\")\n",
    "            type_files[type_list[i]].append(f\"{file} {i}\")\n",
    "            #type_sum[i] += 1\n",
    "            in_type = True\n",
    "            break\n",
    "    if not in_type: \n",
    "        print(file)\n",
    "#print(type_sum)\n",
    "for t in type_list:\n",
    "    print(f\"{t}: {len(type_files[t])} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4187900",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#如何注意不同类别数量的不均衡？\n",
    "#一、SMOTE：通过合成新的样本来增加少数类样本，从而平衡训练集。适用于数据不平衡的情况，使用 imblearn 库中的 SMOTE 方法。\n",
    "#二、加权损失函数：通过加大少数类的损失权重，迫使模型更多地关注少数类。可以使用 Keras 提供的 class_weight 参数，或者自定义加权损失函数如 Focal Loss。\n",
    "#先等比例的来吧\n",
    "# 分割数据的函数\n",
    "def split_data(files, train_ratio):\n",
    "    # 打乱数据\n",
    "    random.shuffle(files)\n",
    "    # 按比例分割\n",
    "    train_size = int(len(files) * train_ratio)\n",
    "    train_files = files[:train_size]\n",
    "    valid_files = files[train_size:]\n",
    "    return train_files, valid_files\n",
    "\n",
    "train_ratio = 0.7\n",
    "train_files = {t: [] for t in type_list}\n",
    "valid_files = {t: [] for t in type_list}\n",
    "#random.seed(17) #这个seed似乎对于shuffle完全没用呢\n",
    "for t in type_list:\n",
    "    train_files[t], valid_files[t] = split_data(type_files[t],train_ratio)\n",
    "    print(f\"{t}: {len(train_files[t])},{len(valid_files[t])} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328bd18",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 合并每类文件路径，并写入 train_txt 和 valid_txt\n",
    "def merge_dict_to_list(file_dict, output):\n",
    "    for category, file_list in file_dict.items():\n",
    "        for file in file_list:\n",
    "            output.append(file)\n",
    "\n",
    "# 写入 train.txt 和 valid.txt\n",
    "train_txt = []\n",
    "valid_txt = []\n",
    "merge_dict_to_list(train_files,train_txt)\n",
    "random.shuffle(train_txt)\n",
    "with open(\"train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(train_txt))\n",
    "merge_dict_to_list(valid_files,valid_txt)\n",
    "random.shuffle(valid_txt)\n",
    "with open(\"valid.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(valid_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8a590d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_data = wsi_dataset(file_path = 'train.txt',transform = data_transforms['train'])\n",
    "valid_data = wsi_dataset(file_path = 'valid.txt',transform = data_transforms['valid'])\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True) # 64张图片为一个包,\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=64, shuffle=True) #GPU上建议始终设置pin_memory=True,\n",
    "\n",
    "'''展示训练数据集中的图片'''\n",
    "# from matplotlib import pyplot as plt\n",
    "# image, Label = iter(train_dataloader).__next__( ) #iter是一个迭代器函数。__next_（）用于获取下一个数据\n",
    "# sample = image[2] #image\n",
    "# sample = sample.permute((1, 2, 0)).numpy() #tensor数据的维度转换\n",
    "# plt.imshow(sample)\n",
    "# plt.show()\n",
    "# print('Label is: {}'.format(Label[2].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef226d7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''定义神经网络'''\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):# 輸入大小(3, 256, 256)\n",
    "        super(CNN, self).__init__( )\n",
    "        self.conv1 = nn.Sequential( #将多个层组合成一起。\n",
    "            nn.Conv2d( #2d一般用于图像，3d用于视频数据（多一个时间维度），1d一般用于结构化的序列数据d\n",
    "                in_channels=3,# 图像通道个数，1表示灰度图（确定了卷积核 组中的个数），\n",
    "                out_channels=16,# 要得到几多少个特征图，卷积核的个数\n",
    "                kernel_size = 5,# 卷积核大小，5*5\n",
    "                stride=1,# 步长\n",
    "                padding=2,#一般希望卷积核处理后的结果大小与处理前的数据大小相同,效果会比较好。那padding改如何设计P\n",
    "            ), #输出的特征图为(16, 256, 256)\n",
    "            nn.ReLU(), # relu层\n",
    "            nn.MaxPool2d(kernel_size=2), # 进行池化操作（2x2 区域），输出结果为：(16, 128, 128)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential( #输入 (16, 128, 128)\n",
    "            nn.Conv2d(16, 32, 5, 1, 2), # 输出（32, 128, 128)\n",
    "            nn.ReLU(), # relu层\n",
    "            nn.Conv2d(32, 32, 5, 1, 2), # 输出(32, 128, 128)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), # 输出(32, 64, 64)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential( #输入 (32, 64, 64)\n",
    "            nn.Conv2d(32, 64, 5, 1, 2),\n",
    "            nn.ReLU(), # 输出(64, 64, 64)\n",
    "        )\n",
    "        self.out = nn.Linear(64 * 64 * 64, 4) #全连接层得到的结果\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)# 输出(64,64, 32, 32)\n",
    "        x = x.view(x.size(0), -1) # flatten操作，结果为： (batch_size, 64 * 32 * 32)\n",
    "        output = self.out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe41430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "#pytorch提供2种方式来切换训练和测试的模式，分别是: model.train()和model.eval()。\n",
    "# 一般用法是：在训练开始之前写上model.trian()，在测试时写上 model.eval() 。\n",
    "#    batch_size_num = 1\n",
    "    for X, y in dataloader: #其中batch为每一个数据的编号\n",
    "        X, y = X.to(device), y.to(device) #把训练数据集和标签传入cpu或GPU\n",
    "        pred = model.forward(X) #自动初始化 w权值\n",
    "        loss = loss_fn(pred, y) #通过交叉熵损失函数计算损失值Loss\n",
    "        # Backpropagation 进来一个batch的数据，计算一次梯度，更新一次网络\n",
    "        optimizer.zero_grad() #梯度值清零\n",
    "        loss.backward() #反向传播计算得到每个参数的梯度值\n",
    "        optimizer.step() #根据梯度更新网络参数\n",
    "\n",
    "        # Loss = loss.item() #获取损失值\n",
    "        # print(f\"Loss: {Loss:>7f} [number:{batch_size_num}]\")\n",
    "        # batch_size_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd338ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    global best_acc\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad(): #一个上下文管理器，关闭梯度计算。当你确认不会调用Tensor.backward()的时候。这可以减少计算所用内存消耗。\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model.forward(X)\n",
    "            test_loss += loss_fn(pred, y).item() #\n",
    "            #print(X,y,pred.argmax(1),sep=\"||\")\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            #a = (pred.argmax(1) == y) #dim=1表示每一行中的最大值对应的索引号， dim=0表示每一列中的最大值对应的索引号\n",
    "            #b = (pred.argmax(1) == y).type(torch.float)\n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test result: \\n Accuracy: {(100*correct)}%, Avg loss: {test_loss}\")\n",
    "        acc_s.append (correct)\n",
    "        loss_s.append(test_loss)     \n",
    "        if correct > best_acc:\n",
    "            best_acc = correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179d20db",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''cnn卷积神经网络部分'''\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "model = resnet_model.to(device)#不会创建一个新对象，而是让 model 和 resnet_model 指向同一个模型。修改 model 会直接影响 resnet_model，包括训练时的权重更新。\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params_to_update,lr=0.001)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.001)#创建一个优化器， SGD为随机梯度下降算法？?\n",
    "#scheduler = torch.optim.Lr_scheduler.stepLR(optimizer,step_size=25,gamma=0.5)\n",
    "'''训练模型'''\n",
    "best_acc = 0\n",
    "epochs = 20\n",
    "acc_s = []\n",
    "loss_s = []\n",
    "for t in range(epochs):\n",
    "    start_time = time.time()\n",
    "    #train_dataloader = DataLoader(training_data,batch_size=64,shuffle=True)\n",
    "    #test_dataloader = DataLoader(test_data,batch_size=64,shuffle=True)\n",
    "    print(f\"Epoch {t+1}\\n--------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    torch.save(model.state_dict(), \"stage1\\model_stage1_epochs_\"+str(t)+\"_train.pth\")\n",
    "    test(valid_dataloader, model, loss_fn)\n",
    "    #保存模型权重\n",
    "    torch.save(model.state_dict(), \"stage1\\model_stage1_epochs_\"+str(t)+\"_test.pth\")\n",
    "    #scheduler.step() #可用来自动调整学习率\n",
    "    end_time = time.time()\n",
    "    time_diff = end_time - start_time\n",
    "    print(\"时间差：\", time_diff)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b443ba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n",
    "#2.5.1+cu118\n",
    "#True\n",
    "#11.8\n",
    "#90100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6f03e8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 创建子图,分别用于绘制准确率和损失值\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(0, epochs), acc_s)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(0, epochs), loss_s)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show() # 显示绘制的图形\n",
    "print(\"Done!\") # 训练结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a265d899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需预测文件路径等预处理\n",
    "def pre_txt_path(image_dir,tile_dir,txt_dir,wsi_type):\n",
    "    index_data = []\n",
    "    # 遍历 image_dir 下的所有 .mrxs 文件\n",
    "    #image_paths = glob.glob(os.path.join(image_dir, \"*\"+wsi_type))\n",
    "    image_paths = glob.glob(f\"{image_dir}/**/*{wsi_type}\", recursive=True)\n",
    "    # 保存每个 image_path 对应的子文件信息\n",
    "    for image_path in image_paths:\n",
    "        # 提取子文件夹名称（基于文件名，不含扩展名）\n",
    "        subfolder_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        subfolder_path = os.path.join(tile_dir, subfolder_name)\n",
    "        # 如果子文件夹不存在，则跳过\n",
    "        if not os.path.isdir(subfolder_path):\n",
    "            print(f\"Subfolder not found for: {image_path}\")\n",
    "            index_data.append({'image_path': image_path, 'txt_path': 'NA'})\n",
    "            continue\n",
    "        # 获取子文件夹中所有 tile*.png 文件\n",
    "        tile_files = glob.glob(os.path.join(subfolder_path, \"tile*.png\"))\n",
    "        # 如果没有匹配的 tile 文件，跳过\n",
    "        if not tile_files:\n",
    "            print(f\"No tile files found in: {subfolder_path}\")\n",
    "            continue\n",
    "        # 构造 DataFrame\n",
    "        sub_df = pd.DataFrame({\n",
    "            \"tile_file_name\": tile_files,\n",
    "            \"label\": -1  # 所有文件初始标签为 -1\n",
    "        })\n",
    "        # 保存为 txt 文件\n",
    "        output_txt = os.path.join(txt_dir, f\"{subfolder_name}.txt\")\n",
    "        index_data.append({'image_path': image_path, 'txt_path': output_txt})\n",
    "        sub_df.to_csv(output_txt, sep=\" \", index=False, header=False)\n",
    "        print(f\"Sub_df saved for {image_path} to {output_txt}\")\n",
    "    index_df = pd.DataFrame(index_data)\n",
    "    # 保存索引文件\n",
    "    index_df.to_csv(os.path.join(txt_dir, \"index.csv\"), index=False)\n",
    "    print(f\"Index file saved to {os.path.join(txt_dir, 'index.csv')}\")\n",
    "\n",
    "#pre_txt_path(image_dir = r\"E:\\THT\\HE_DTC\", \n",
    "#             tile_dir = r\"E:\\THT\\HE_DTC_try\",\n",
    "#             txt_dir = r\"E:\\THT\\HE_DTC_txt\",\n",
    "#             wsi_type = \".mrxs\")\n",
    "#pre_txt_path(image_dir = r\"E:\\THT\\HE_DTC_TCGA\\slides\", \n",
    "#             tile_dir = r\"E:\\THT\\HE_DTC_TCGA\\cut\",\n",
    "#             txt_dir = r\"E:\\THT\\HE_DTC_TCGA\\txt\",\n",
    "#             wsi_type = \".svs\")\n",
    "pre_txt_path(image_dir = r\"E:\\THT\\HE_DTC_second\\second\\mrxs\", \n",
    "             tile_dir = r\"E:\\THT\\HE_DTC_second\\second\\cut_123\",\n",
    "             txt_dir = r\"E:\\THT\\HE_DTC_second\\second\\txt_123\",\n",
    "             wsi_type = \".mrxs\")\n",
    "#print(\"------------我是分界线------------\")\n",
    "pre_txt_path(image_dir = r\"E:\\THT\\HE_DTC_second\\second\\kfb\\svs\", \n",
    "             tile_dir = r\"E:\\THT\\HE_DTC_second\\second\\cut_pudong\",\n",
    "             txt_dir = r\"E:\\THT\\HE_DTC_second\\second\\txt_pudong\",\n",
    "             wsi_type = \".svs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee439f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#读分割好的tiles by slide，输出预测labels.csv   #可视化\n",
    "#onco-fusion/tissue-type-training/eval_tissue_tile.py/def _visualize\n",
    "def get_color_from_class(pred_class):\n",
    "    \"\"\"\n",
    "    根据预测类别返回对应的颜色（这里可以根据需要自定义）。\n",
    "    Args:\n",
    "        pred_class (int): 预测的类别。\n",
    "    Returns:\n",
    "        tuple: 对应的颜色（RGB）。\n",
    "    \"\"\"\n",
    "    color_map = {\n",
    "        0: (255, 0, 0),  # 类别 0 - 红色\n",
    "        1: (0, 255, 0),  # 类别 1 - 绿色\n",
    "        2: (0, 0, 255),  # 类别 2 - 蓝色\n",
    "        3: (255, 255, 0),  # 类别 3 - 黄色\n",
    "    }\n",
    "    return color_map.get(pred_class, (0, 0, 0))  # 默认黑色（如果类别不在 map 中）\n",
    "    \n",
    "def make_preds_by_slide_and_visualize(model, index_df, device, output_dir, transform, \n",
    "                                      visualize=True, out_all=True, all_shuffle=True,\n",
    "                                      batch_size=64, num_workers=28, n_classes=4, scale_factor=1, \n",
    "                                      wsi_type=\".mrxs\",sum_calc=False,pick_label=-1):\n",
    "    \"\"\"\n",
    "    根据每个瓦片的预测结果生成完整图像。\n",
    "    Args:\n",
    "        model (torch.nn.Module): 训练好的模型。\n",
    "        index_df (pd.DataFrame): 包含 'image_path' 和 'txt_path' 的索引文件。\n",
    "        output_dir (str): 结果保存的目录。\n",
    "        transform (callable): 图像转换函数（如数据增强）。\n",
    "        device (torch.device): 用于运行模型的设备 (CPU 或 GPU)。\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "    #消除PIL库image最大像素限制\n",
    "    Image.MAX_IMAGE_PIXELS = 230000000000\n",
    "    \n",
    "    def scale(x,scale_factor):\n",
    "        return round(x / scale_factor)\n",
    "\n",
    "    # 定义 CSV 表头\n",
    "    header = ['tile_file_name', 'label']\n",
    "    header.extend(['score_{}'.format(k) for k in range(n_classes)])\n",
    "    # 合并生成一个txt\n",
    "    all_file = []\n",
    "    if sum_calc:\n",
    "        sum_all = [['wsi_name','out_label']] #,*range(n_classes)]] #* --> 解包\n",
    "        sum_all[0].extend((\" \".join(f\"{i} {i}(%)\" for i in range(n_classes))).split(\" \"))\n",
    "    if pick_label>-1:\n",
    "        pick_path = []\n",
    "        pick_dir = os.path.join(output_dir,\"txt\")\n",
    "        os.makedirs(pick_dir, exist_ok=True)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for _, row in index_df.iterrows():\n",
    "        image_path = row['image_path']  # 获取原图路径\n",
    "        image_name = os.path.basename(image_path)\n",
    "        txt_path = row['txt_path']      # 获取对应的 txt 文件路径\n",
    "        # 如果 txt 文件不存在，跳过\n",
    "        if (txt_path != txt_path) or (not os.path.exists(txt_path)): # 注意np.nan != np.nan\n",
    "            print(f\"Warning: {txt_path} not found. Skipping...\")\n",
    "            continue\n",
    "        # 创建 Dataset 和 DataLoader\n",
    "        dataset = wsi_dataset(file_path=txt_path, transform=transform)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, \n",
    "                                num_workers=num_workers, pin_memory=True, shuffle=False)\n",
    "        if visualize:\n",
    "            # 读取原始图像尺寸信息\n",
    "            slide = Slide(image_path,output_dir)\n",
    "            slide_width, slide_height = slide.dimensions\n",
    "            if wsi_type==\".svs\":\n",
    "                slide_height, slide_width = slide.dimensions\n",
    "            #按照后缀分没用，有正有歪的 #还是放个正方形来看吧\n",
    "            if slide_width>slide_height:\n",
    "                slide_height = slide_width\n",
    "            else:\n",
    "                slide_width = slide_height\n",
    "            # 创建一个空白图像，大小为原始图像的大小，背景为黑色\n",
    "            #full_image = Image.new('RGB', (scale(slide_width,scale_factor), scale(slide_height,scale_factor)), (0, 0, 0))\n",
    "            #full_image_array = np.array(full_image)\n",
    "            full_image_array = np.zeros((scale(slide_width,scale_factor), scale(slide_height,scale_factor), 3), dtype=np.uint8)\n",
    "\n",
    "        # 创建输出文件\n",
    "        output_file = os.path.join(output_dir, os.path.basename(image_path).replace(wsi_type, '.csv'))\n",
    "        if sum_calc:\n",
    "            sum_list = [0 for i in range(n_classes)]\n",
    "        if pick_label>-1:\n",
    "            pick_list = []\n",
    "        \n",
    "        with open(output_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file, delimiter=',')\n",
    "            writer.writerow(header)\n",
    "            # 模型预测\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx,(X,y) in enumerate(dataloader):\n",
    "                #for X,y in dataloader:\n",
    "                    X = X.to(device)\n",
    "                    # 获取预测结果\n",
    "                    preds = model(X)\n",
    "                    preds = preds.detach().cpu().numpy()\n",
    "                    #print(len(preds)) #==batch_size\n",
    "                    #print(batch_idx)\n",
    "                    #labels = labels.cpu().tolist()\n",
    "                    img_paths = dataset.imgs[batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
    "                    for img_path,pred_list in zip(img_paths,preds):\n",
    "                    #for img_path, pred_list in zip(dataset.imgs, preds):\n",
    "                        #print(img_path,pred_list)\n",
    "                        # 获取预测类别，并根据类别填充颜色\n",
    "                        pred_class = pred_list.argmax()  # 选择最大概率的类别\n",
    "                        color = get_color_from_class(pred_class)\n",
    "                        img_name = os.path.basename(img_path)\n",
    "                        #print(pred_list,pred_class,img_name)\n",
    "                        # 写入每个瓦片的预测结果\n",
    "                        #print(img_path,pred_class)\n",
    "                        write_row = [img_path,pred_class]\n",
    "                        write_row.extend(pred_list)\n",
    "                        writer.writerow(write_row)\n",
    "                        all_file.append(f\"{img_path} {pred_class}\")\n",
    "                        if pick_label > -1:\n",
    "                            if int(pred_class)==int(pick_label):\n",
    "                                pick_list.append(f\"{img_path} -1\")\n",
    "                        if sum_calc:\n",
    "                            sum_list[int(pred_class)] += 1\n",
    "                        # 填充瓦片位置\n",
    "                        if visualize:\n",
    "                            # 获取瓦片的左上角和右下角坐标\n",
    "                            x1, y1, x2, y2 = map(int, img_name.rsplit('_', 1)[-1].replace('.png', '').split('-'))\n",
    "                            #print(x1,y1,x2,y2)\n",
    "                            full_image_array[scale(y1,scale_factor):scale(y2,scale_factor), scale(x1,scale_factor):scale(x2,scale_factor)] = color\n",
    "        #print(f\"Predictions for {image_path} saved to {output_file}\")\n",
    "        if pick_label > -1:\n",
    "            pick_txt = os.path.join(pick_dir, os.path.basename(txt_path))\n",
    "            pick_path.append({'image_path': image_path, 'txt_path': pick_txt})\n",
    "            with open(pick_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"\\n\".join(pick_list))\n",
    "        if sum_calc:\n",
    "            sum_single = [image_name,str(sum_list.index(max(sum_list)))]\n",
    "            sum_file = os.path.join(output_dir, os.path.basename(image_path).replace(wsi_type,'_sum_out_label='+str(sum_list.index(max(sum_list)))+'.txt'))\n",
    "            with open(sum_file, \"w\") as f:\n",
    "                total = sum(sum_list)\n",
    "                percentages = [(x / total) * 100 for x in sum_list]\n",
    "                f.write(\"\\n\\n统计结果：\\n\")\n",
    "                for i, (count, percent) in enumerate(zip(sum_list, percentages)):\n",
    "                    f.write(f\"类别 {i}: 数量 {count}, 百分比 {percent:.2f}%\\n\")\n",
    "                    sum_single.append(str(count))\n",
    "                    sum_single.append(f\"{percent:.2f}%\")\n",
    "                f.write(f\"\\n总数: {total}\\n\")\n",
    "            sum_all.append(sum_single)\n",
    "            #print(f\"Sum of predictions saved to {sum_file}\")\n",
    "        if visualize:\n",
    "            # 将完整图像从 NumPy 数组转换回 PIL 图像\n",
    "            #full_image = Image.fromarray(full_image_array)\n",
    "            full_image = Image.fromarray(full_image_array.astype(np.uint8))\n",
    "            # 保存完整图像\n",
    "            output_image_path = os.path.join(output_dir, os.path.basename(image_path).replace(wsi_type, '_full_pred.png'))\n",
    "            #plt.imshow(full_image_array)\n",
    "            #plt.title(\"Generated Image Preview\")\n",
    "            #plt.show()\n",
    "            full_image.save(output_image_path, format='PNG')\n",
    "            #full_image.thumbnail((1024, 1024))  # 生成缩略图进行调试\n",
    "            #full_image.save(output_image_path.replace('.png','_thumbnail_1024.png'), format='PNG')\n",
    "            #print(f\"Generated and saved visualization to {output_image_path}\")\n",
    "    if all_shuffle:\n",
    "        random.shuffle(all_file)\n",
    "    if out_all:\n",
    "        all_txt = os.path.join(output_dir, \"all_file.txt\")\n",
    "        with open(all_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(all_file))\n",
    "    if sum_calc:\n",
    "        sum_csv = os.path.join(output_dir,\"sum_all.csv\")\n",
    "        with open(sum_csv, \"w\", newline=\"\") as f:  \n",
    "            writer = csv.writer(f)  \n",
    "            writer.writerows(sum_all)\n",
    "    if pick_label>-1:\n",
    "        pick_index_df = pd.DataFrame(pick_path)\n",
    "        pick_index_df.to_csv(os.path.join(pick_dir, \"pick_index.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d605a7e1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "index_df = pd.read_csv(r'E:\\THT\\HE_DTC_txt\\index.csv')\n",
    "#print(index_df[\"image_path\"][:2])\n",
    "#print(index_df.iloc[2])\n",
    "cut_lists = [\"10-03611D\", \"14-08954B\",\"15-04187\",\"10-16623C\",\n",
    "             \"15-41593G\",\"16-03266F\",\"16-06309\",\"16-11786C\",\n",
    "             \"16-39386B\",\"16-54346D\",\"17-03704B\",\"17-04172B\",\"17-25206B\",\n",
    "             \"17-42804B\",\"18-14810B\",\"18-29417D\",\"19-45599D\"]\n",
    "cut_mrxs_lists = ['E:\\\\THT\\\\HE_DTC\\\\'+x+\".mrxs\" for x in cut_lists]\n",
    "#print(cut_mrxs_lists)\n",
    "df_filtered = index_df[index_df['image_path'].isin(cut_mrxs_lists)]\n",
    "print(len(df_filtered))\n",
    "# 剩余的行\n",
    "df_remaining = index_df[~index_df['image_path'].isin(cut_mrxs_lists)]\n",
    "print(len(df_remaining))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca6862c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "index_df = pd.read_csv(r'E:\\THT\\HE_DTC_txt\\index.csv')\n",
    "make_preds_by_slide_and_visualize(model_epoch_10, df_filtered, device, r\"E:\\THT\\HE_DTC_output\",visualize=True,out_all=False,\n",
    "                                  transform=data_transforms['valid'], batch_size=64, num_workers=0, n_classes=4, scale_factor=80)\n",
    "\n",
    "#index_df = pd.read_csv(r'E:\\THT\\HE_DTC_txt\\index.csv')\n",
    "#index_two = index_df[-6:-4]\n",
    "#print(index_df[-6:-4])\n",
    "#make_preds_by_slide_and_visualize(model_epoch_10, index_two, device, r\"E:\\THT\\HE_DTC_output\\remain_visual\",visualize=False,out_all=False,\n",
    "#                                  transform=data_transforms['valid'], batch_size=64, num_workers=0, n_classes=4, scale_factor=1)\n",
    "#到时候 首先把已经用过的训练/验证的切片去除掉 index_df[16:]\n",
    "#其次，输出最后放在一个txt里，[path,pred] 不用可视化了，\n",
    "make_preds_by_slide_and_visualize(model_epoch_10, df_remaining, device, r\"E:\\THT\\HE_DTC_output\\remain\",visualize=True,out_all=True,\n",
    "                                  transform=data_transforms['valid'], batch_size=64, num_workers=0, n_classes=4, scale_factor=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fb210d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#增加蛋白分型标签，选取肿瘤对应样本\n",
    "#tumor-1 == case4\n",
    "#18-27078 == 18-27018\n",
    "#13-03990 == 13-03980\n",
    "protein_label_path = r\"E:\\THT\\DTC_histology\\protein_label.txt\"\n",
    "index_df = pd.read_csv(r'E:\\THT\\HE_DTC_txt\\index.csv')\n",
    "wsi_origin = [index_df.iloc[x][0] for x in range(len(index_df))]\n",
    "wsi_origin = [x[x.rfind(\"\\\\\")+1:x.rfind(\".\")] for x in wsi_origin]\n",
    "#print(wsi_origin)\n",
    "wsi_list = [re.sub(r'[A-Z\\s]', '', index_df.iloc[x][0]) for x in range(len(index_df))]\n",
    "wsi_list = [x[x.rfind(\"\\\\\")+1:x.rfind(\".\")] for x in wsi_list]\n",
    "protein_list = []\n",
    "protein_dict = {}\n",
    "with open(protein_label_path) as f:\n",
    "    protein_labels = [x[:-1] for x in f.readlines()]\n",
    "for i in range(len(protein_labels)//2):\n",
    "    protein_list.append((protein_labels[i][2:],protein_labels[i+len(protein_labels)//2]))\n",
    "#print(wsi_list,protein_list)\n",
    "protein_name = [x[0]for x in protein_list]\n",
    "for i in wsi_list:\n",
    "    if i not in protein_name:\n",
    "        print(i)\n",
    "for i in range(len(wsi_list)):\n",
    "    for j in range(len(protein_name)):\n",
    "        if wsi_list[i]==protein_name[j]:\n",
    "            protein_dict[wsi_origin[i]] = int(protein_list[j][1])\n",
    "print(protein_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241a5db5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#增加蛋白分型标签，选取肿瘤对应样本\n",
    "#filtered --> 17 txts --> all_files.txt\n",
    "#filtered_txt = df_filtered[\"txt_path\"]\n",
    "#print(filtered_txt.iloc[1])\n",
    "filtered_file_path = \"all_files.txt\"\n",
    "#remain --> all_file.txt\n",
    "remain_file_path = r\"E:\\THT\\HE_DTC_output\\remain\\all_file.txt\"\n",
    "tumor_txts_path = [filtered_file_path,remain_file_path]\n",
    "#print(tumor_txts_path)\n",
    "tumor_tiles_with_protein = \"tumor_tiles_with_protein.txt\"\n",
    "tumor_tiles_list = []\n",
    "protein_labels_list = []\n",
    "sep_list = [\"cut\",\"try\"] #两文件路径还不一样的哎\n",
    "none_list = []\n",
    "#with open(tumor_tiles_with_protein, \"w\", encoding=\"utf-8\") as wf:\n",
    "#    for i in range(len(tumor_txts_path)):\n",
    "#        with open(tumor_txts_path[i], 'r', encoding='utf-8') as rf:\n",
    "#            for line in rf:\n",
    "#                tile_with_type = line\n",
    "#                tile_with_protein = line[:-2]\n",
    "#                tile_wsi_path = tile_with_type[tile_with_type.rfind(sep_list[i])+4:tile_with_type.rfind(\"tile\")]\n",
    "#                tile_wsi_name = tile_wsi_path[:tile_wsi_path.find('\\\\')]\n",
    "#                #print(tile_with_type[-2]==\"1\", tile_with_protein, tile_wsi_path, tile_wsi_name)\n",
    "#                if tile_with_type[-2]==\"0\":\n",
    "#                    tile_with_protein = tile_with_protein\n",
    "#                    tile_protein_type = protein_dict.get(tile_wsi_name,protein_dict.get(tile_wsi_name[:-1],0))-1\n",
    "#                    if tile_protein_type==-1:\n",
    "#                        if tile_wsi_name not in none_list:\n",
    "#                            none_list.append(tile_wsi_name)\n",
    "#                    else:\n",
    "#                        tumor_tiles_list.append(tile_with_protein[:-1])\n",
    "#                        protein_labels_list.append(str(tile_protein_type))\n",
    "#                        wf.write(tile_with_protein+str(tile_protein_type)+\"\\n\")\n",
    "#不写入txt 只存入list\n",
    "for i in range(len(tumor_txts_path)):\n",
    "    with open(tumor_txts_path[i], 'r', encoding='utf-8') as rf:\n",
    "        for line in rf:\n",
    "            tile_with_type = line\n",
    "            tile_with_protein = line[:-2]\n",
    "            tile_wsi_path = tile_with_type[tile_with_type.rfind(sep_list[i])+4:tile_with_type.rfind(\"tile\")]\n",
    "            tile_wsi_name = tile_wsi_path[:tile_wsi_path.find('\\\\')]\n",
    "            if tile_with_type[-2]==\"0\":\n",
    "                tile_with_protein = tile_with_protein\n",
    "                tile_protein_type = protein_dict.get(tile_wsi_name,protein_dict.get(tile_wsi_name[:-1],0))-1\n",
    "                if tile_protein_type==-1:\n",
    "                    if tile_wsi_name not in none_list:\n",
    "                        none_list.append(tile_wsi_name)\n",
    "                else:\n",
    "                    tumor_tiles_list.append(tile_with_protein[:-1])\n",
    "                    protein_labels_list.append(str(tile_protein_type))              \n",
    "print(none_list)\n",
    "X_train, X_test, y_train, y_test = train_test_split(tumor_tiles_list, protein_labels_list, test_size=0.3, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3491f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#分型模型 训练/验证集\n",
    "with open(\"protein_train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(len(X_train)):\n",
    "        if i==len(X_train):\n",
    "            f.write(X_train[i]+\" \"+y_train[i])\n",
    "        else:\n",
    "            f.write(X_train[i]+\" \"+y_train[i]+\"\\n\")\n",
    "with open(\"protein_valid.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(len(X_test)):\n",
    "        if i==len(X_test):\n",
    "            f.write(X_test[i]+\" \"+y_test[i])\n",
    "        else:\n",
    "            f.write(X_test[i]+\" \"+y_test[i]+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311795c4-0a4d-4233-a71f-3780b01ce95e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#根据人 分割训练/验证集 #p for person\n",
    "resum_dict = resum_data(get_name(tumor_tiles_list,cut_pre=\"\\\\\",cut_end=\"\\\\t\"),protein_labels_list)\n",
    "pX_train, pX_test, py_train, py_test = train_test_split(list(resum_dict.keys()),\n",
    "                                                        [x for y in list(resum_dict.values()) for x in y], \n",
    "                                                        test_size=0.3, random_state=17)\n",
    "with open(\"protein_train_by_person.txt\", \"w\", encoding=\"utf-8\") as ft:\n",
    "    with open(\"protein_valid_by_person.txt\", \"w\", encoding=\"utf-8\") as fv:\n",
    "        for i in range(len(tumor_tiles_list)):\n",
    "            is_in_train = False\n",
    "            is_in_valid = False\n",
    "            for j in pX_train:\n",
    "                if j in tumor_tiles_list[i]:\n",
    "                    is_in_train = True\n",
    "                    break\n",
    "            for j in pX_test:\n",
    "                if j in tumor_tiles_list[i]:\n",
    "                    is_in_valid = True\n",
    "                    break\n",
    "            if not is_in_train^is_in_valid:\n",
    "                print(tumor_tiles_list[i])\n",
    "            if i==len(tumor_tiles_list):\n",
    "                if is_in_train:\n",
    "                    ft.write(tumor_tiles_list[i]+\" \"+protein_labels_list[i])\n",
    "                else:\n",
    "                    fv.write(tumor_tiles_list[i]+\" \"+protein_labels_list[i])\n",
    "            else:\n",
    "                if is_in_train:\n",
    "                    ft.write(tumor_tiles_list[i]+\" \"+protein_labels_list[i]+\"\\n\")\n",
    "                else:\n",
    "                    fv.write(tumor_tiles_list[i]+\" \"+protein_labels_list[i]+\"\\n\")\n",
    "\n",
    "X_train, y_train = reget_data(\"protein_train_by_person.txt\")\n",
    "X_test, y_test = reget_data(\"protein_valid_by_person.txt\")\n",
    "print(set(list(resum_data(get_name(X_train,cut_pre=\"\\\\\",cut_end=\"\\\\t\"),y_train).keys()))&set(list(resum_data(get_name(X_test,cut_pre=\"\\\\\",cut_end=\"\\\\t\"),y_test).keys())))\n",
    "print(set(list(resum_data(get_name(X_train,cut_pre=\"\\\\\",cut_end=\"\\\\t\"),y_train).keys()))|set(list(resum_data(get_name(X_test,cut_pre=\"\\\\\",cut_end=\"\\\\t\"),y_test).keys())))\n",
    "print(len(set(list(resum_data(get_name(X_train,cut_pre=\"\\\\\",cut_end=\"\\\\t\"),y_train).keys()))|set(list(resum_data(get_name(X_test,cut_pre=\"\\\\\",cut_end=\"\\\\t\"),y_test).keys()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f5e477",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "protein_train_data = wsi_dataset(file_path = 'protein_train_by_person.txt',transform = data_transforms['train'])\n",
    "protein_valid_data = wsi_dataset(file_path = 'protein_valid_by_person.txt',transform = data_transforms['valid'])\n",
    "\n",
    "protein_train_dataloader = DataLoader(protein_train_data, batch_size=64, shuffle=True,\n",
    "                                      #num_workers=4,\n",
    "                                      #persistent_workers=True,\n",
    "                                      pin_memory=True) # 64张图片为一个包,\n",
    "protein_valid_dataloader = DataLoader(protein_valid_data, batch_size=64, shuffle=True,\n",
    "                                      #num_workers=4,\n",
    "                                      #persistent_workers=True,\n",
    "                                      pin_memory=True) #GPU上建议始终设置pin_memory=True,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3349507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):# 輸入大小(3, 256, 256)\n",
    "        super(CNN, self).__init__( )\n",
    "        self.conv1 = nn.Sequential( #将多个层组合成一起。\n",
    "            nn.Conv2d( #2d一般用于图像，3d用于视频数据（多一个时间维度），1d一般用于结构化的序列数据d\n",
    "                in_channels=3,# 图像通道个数，1表示灰度图（确定了卷积核 组中的个数），\n",
    "                out_channels=16,# 要得到几多少个特征图，卷积核的个数\n",
    "                kernel_size = 5,# 卷积核大小，5*5\n",
    "                stride=1,# 步长\n",
    "                padding=2,#一般希望卷积核处理后的结果大小与处理前的数据大小相同,效果会比较好。那padding改如何设计P\n",
    "            ), #输出的特征图为(16, 256, 256)\n",
    "            nn.ReLU(), # relu层\n",
    "            nn.MaxPool2d(kernel_size=2), # 进行池化操作（2x2 区域），输出结果为：(16, 128, 128)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential( #输入 (16, 128, 128)\n",
    "            nn.Conv2d(16, 32, 5, 1, 2), # 输出（32, 128, 128)\n",
    "            nn.ReLU(), # relu层\n",
    "            nn.Conv2d(32, 32, 5, 1, 2), # 输出(32, 128, 128)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), # 输出(32, 64, 64)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential( #输入 (32, 64, 64)\n",
    "            nn.Conv2d(32, 64, 5, 1, 2),\n",
    "            nn.ReLU(), # 输出(64, 64, 64)\n",
    "        )\n",
    "        self.out = nn.Linear(64 * 64 * 64, 3) #全连接层得到的结果\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)# 输出(64,64, 32, 32)\n",
    "        x = x.view(x.size(0), -1) # flatten操作，结果为： (batch_size, 64 * 32 * 32)\n",
    "        output = self.out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd2e6d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#分型模型 #冻结仅57%,尝试不冻结\n",
    "'''cnn卷积神经网络部分'''\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "resnet_model,params_to_update = model_initialization(dimension=3,freeze=False)\n",
    "model = resnet_model.to(device)#不会创建一个新对象，而是让 model 和 resnet_model 指向同一个模型。修改 model 会直接影响 resnet_model，包括训练时的权重更新。\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(params_to_update,lr=0.001)\n",
    "# 修改优化器配置（增加权重衰减）\n",
    "optimizer = torch.optim.Adam(params_to_update, lr=0.003, weight_decay=1e-4)\n",
    "# 添加学习率调度器（根据验证损失调整）\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.001)#创建一个优化器， SGD为随机梯度下降算法？?\n",
    "#scheduler = torch.optim.Lr_scheduler.stepLR(optimizer,step_size=25,gamma=0.5)\n",
    "'''训练模型'''\n",
    "best_acc = 0\n",
    "epochs = 20\n",
    "acc_s = []\n",
    "loss_s = []\n",
    "for t in range(epochs):\n",
    "    start_time = time.time()\n",
    "    #train_dataloader = DataLoader(training_data,batch_size=64,shuffle=True)\n",
    "    #test_dataloader = DataLoader(test_data,batch_size=64,shuffle=True)\n",
    "    print(f\"Epoch {t+1}\\n--------------------\")\n",
    "    train(protein_train_dataloader, model, loss_fn, optimizer)\n",
    "    #保存模型权重\n",
    "    torch.save(model.state_dict(), \"stage2_2\\model_stage2_epochs_\"+str(t)+\".pth\")\n",
    "    test(protein_valid_dataloader, model, loss_fn)\n",
    "    scheduler.step(loss_s[-1]) #可用来自动调整学习率\n",
    "    end_time = time.time()\n",
    "    time_diff = end_time - start_time\n",
    "    print(\"时间差：\", time_diff)\n",
    "    # 早停机制（连续3次未提升则停止）\n",
    "    if t > 10 and best_acc not in acc_s[-3:]:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454b850f-b0ec-4b4f-ae68-069557fede3b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 创建子图,分别用于绘制准确率和损失值\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(0, epochs), acc_s)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(0, epochs), loss_s)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show() # 显示绘制的图形\n",
    "print(\"Done!\") # 训练结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34201cd-c5ac-4ceb-ada7-68ea1ec9d7de",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#重新获取训练集和验证集\n",
    "def reget_data(file_path):\n",
    "    X, y = [], []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line[-1]==\"\\n\":\n",
    "                part_0 = line[:-3]\n",
    "                part_1 = line[-2:-1]\n",
    "            else:\n",
    "                part_0 = line[:-2]\n",
    "                part_1 = line[-1:]\n",
    "            X.append(part_0)\n",
    "            y.append(part_1)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 读取训练集和验证集\n",
    "X_train, y_train = reget_data(\"protein_train.txt\")\n",
    "X_test, y_test = reget_data(\"protein_valid.txt\")\n",
    "print(X_train[:3], X_test[:3], y_train[:3], y_test[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caf9d06-f0d8-46bc-8d43-ff991193c84b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#统计训练集和验证集中数目\n",
    "from collections import Counter\n",
    "#print(Counter(y_train))\n",
    "#print(Counter(y_test))\n",
    "def get_name(name_list,cut_pre=\"\",cut_end=\"\"):\n",
    "    name_cut_end = [x[:x.find(cut_end)] for x in name_list]\n",
    "    name_cut = [x[x.rfind(cut_pre)+1:] for x in name_cut_end]\n",
    "    return name_cut\n",
    "\n",
    "def resum_data(name_list,value_list):\n",
    "    value_dict = {}\n",
    "    for name, value in zip(name_list, value_list):\n",
    "        if name not in value_dict:\n",
    "            value_dict[name] = []\n",
    "        if value not in value_dict[name]:\n",
    "            value_dict[name].append(value)\n",
    "    result = {}\n",
    "    is_all_same = True\n",
    "    for name, values in value_dict.items():\n",
    "        # 若名称重复出现且对应多个值，检查一致性\n",
    "        is_same = True\n",
    "        if len(values) > 1:\n",
    "            is_same = len(set(values)) == 1  # 使用集合去重判断值是否一致‌:ml-citation{ref=\"6\" data=\"citationList\"}\n",
    "            print(\"Not same:\",name,values)\n",
    "            is_all_same = False\n",
    "        result[name] = {\n",
    "            \"values\": values,\n",
    "            \"is_same\": is_same\n",
    "        }\n",
    "    print(\"名称列表统计：\",Counter(name_list))\n",
    "    print(\"键值列表统计：\",Counter(value_list))\n",
    "    if is_all_same:\n",
    "        print(\"全子元素统计：\",Counter([value for values in value_dict.values() for value in values]))\n",
    "        print(\"仅子元素统计：\",Counter([tuple(values) for values in value_dict.values()]))\n",
    "        return value_dict\n",
    "    else: \n",
    "        print(\"全子元素统计：\",Counter([value for values in result.values() for value in values[\"values\"]]))\n",
    "        print(\"仅子元素统计：\",Counter([tuple(values[\"values\"]) for values in result.values()]))\n",
    "        return result\n",
    "        \n",
    "print(resum_data(get_name(X_train,cut_pre=\"\\\\\",cut_end=\"\\\\t\"),y_train))\n",
    "print(resum_data(get_name(X_test,cut_pre=\"\\\\\",cut_end=\"\\\\t\"),y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c4398e-579d-4375-b69c-025228f1004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算AUC 绘制ROC\n",
    "resnet_model,params_to_update = model_initialization(dimension=3,freeze=False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = model_load(resnet_model,device,\"stage2_best_54\\model_stage2_epochs_\"+str(13)+\".pth\")\n",
    "model.eval()\n",
    "# 读取测试数据\n",
    "#X_test, y_test = reget_data(\"protein_valid.txt\")\n",
    "#X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)  # 转为 Tensor\n",
    "#y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "# 创建 DataLoader\n",
    "#batch_size = 256\n",
    "#test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(protein_valid_data, batch_size=64, shuffle=False)\n",
    "#protein_valid_dataloader = DataLoader(protein_valid_data, batch_size=64, shuffle=True)\n",
    "# 用于存储真实标签和预测概率的列表\n",
    "y_scores = []\n",
    "y_labels = []\n",
    "#with torch.no_grad(): #一个上下文管理器，关闭梯度计算。当你确认不会调用Tensor.backward()的时候。这可以减少计算所用内存消耗。\n",
    "#    for X, y in test_loader:\n",
    "#        X, y = X.to(device), y.to(device)\n",
    "#        pred = model.forward(X)\n",
    "#        #test_loss += loss_fn(pred, y).item() #\n",
    "#        print(X,y,pred.argmax(1),sep=\"||\")\n",
    "#        #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "#        #break\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)  # 计算输出\n",
    "        probs = torch.softmax(outputs, dim=1)  # [:,1]获取正类（1）的概率,但这是三分类 不适用\n",
    "        y_scores.append(probs.cpu().numpy())  # 存储预测概率\n",
    "        y_labels.append(y_batch.cpu().numpy())  # 存储真实标签\n",
    "y_probs = np.concatenate(y_scores)\n",
    "y_true = np.concatenate(y_labels)\n",
    "y_onehot_true = LabelBinarizer().fit(range(3)).transform(y_true)\n",
    "# 因为你是三分类任务，所以y_probs的形状应该是(num_samples, 3)\n",
    "# y_true的形状应该是(num_samples,)\n",
    "#print(\"-------------------------------------------\")\n",
    "# 计算每个类别的ROC曲线和AUC\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "n_classes = 3\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true == i, y_probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3652fbad-98f0-465d-959e-e908f54e6f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc_auc_score计算宏/微平均AUC\n",
    "print(roc_auc_score(y_true, y_probs, multi_class='ovr',average='macro'))\n",
    "print(roc_auc_score(y_true, y_probs, multi_class='ovo',average='macro'))\n",
    "print(roc_auc_score(y_true, y_probs, multi_class='ovr',average='micro'))\n",
    "print(y_true.shape,y_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a8b0be-a0e2-427b-b536-5f561cd77efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_onehot_true = LabelBinarizer().fit(range(3)).transform(y_true)\n",
    "# Compute micro-average ROC curve and ROC area（方法二）\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_onehot_true.ravel(), y_probs.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    " \n",
    "# Compute macro-average ROC curve and ROC area（方法一）\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    " \n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    " \n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    " \n",
    "# Plot all ROC curves\n",
    "lw=2\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    " \n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    " \n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.4f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    " \n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([-0.02, 1.0])\n",
    "plt.ylim([0.0, 1.02])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0309b9-6af7-4555-acc1-1a989fbf9c77",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#应用model1\n",
    "index_df_123 = pd.read_csv(r'E:\\THT\\HE_DTC_second\\second\\txt_123\\index.csv')\n",
    "index_df_pudong = pd.read_csv(r'E:\\THT\\HE_DTC_second\\second\\txt_pudong\\index.csv')\n",
    "resnet_model_epoch_10,params_no_use = model_initialization(4)\n",
    "model_epoch_10 = model_load(resnet_model_epoch_10,device,\"stage1\\model_stage1_epochs_10_train.pth\")\n",
    "model_epoch_10.eval()\n",
    "make_preds_by_slide_and_visualize(model_epoch_10, index_df_123, device, r\"E:\\THT\\HE_DTC_second\\second\\output\\model1_out\\123\",transform=data_transforms['valid'],\n",
    "                                  visualize=True,out_all=True,all_shuffle=False,batch_size=64, num_workers=0, n_classes=4, scale_factor=80,\n",
    "                                  wsi_type=\".mrxs\",sum_calc=True,pick_label=0)\n",
    "print(\"------------------我是分界线！-------------------\")\n",
    "make_preds_by_slide_and_visualize(model_epoch_10, index_df_pudong, device, r\"E:\\THT\\HE_DTC_second\\second\\output\\model1_out\\pudong\",transform=data_transforms['valid'],\n",
    "                                  visualize=True,out_all=True,all_shuffle=False,batch_size=64, num_workers=0, n_classes=4, scale_factor=80,\n",
    "                                  wsi_type=\".svs\",sum_calc=True,pick_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6320530a-f80a-4f6f-9416-171f8a26e526",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#应用model2 #TCGA\n",
    "#pick_index_df = pd.read_csv(r'E:\\THT\\HE_DTC_second\\second\\output\\model1_out\\123\\txt\\pick_index.csv')\n",
    "#output_dir = r'E:\\THT\\HE_DTC_second\\second\\output\\model2_out\\123'\n",
    "pick_index_df = pd.read_csv(r'E:\\THT\\HE_DTC_TCGA\\output\\model1_out\\txt\\pick_index.csv')\n",
    "output_dir = r'E:\\THT\\HE_DTC_TCGA\\output\\model2_2out'\n",
    "#for t in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19]:\n",
    "#for t in [0,3,6,18]:\n",
    "for t in range(21):\n",
    "    resnet_model,params_to_update = model_initialization(dimension=3,freeze=False)\n",
    "    model = model_load(resnet_model,device,\"stage2_best_54\\model_stage2_epochs_\"+str(t)+\".pth\")\n",
    "    model.eval()\n",
    "    make_preds_by_slide_and_visualize(model, pick_index_df, device, os.path.join(output_dir,str(t)), transform=data_transforms['valid'], \n",
    "                                      visualize=True,out_all=True,all_shuffle=False,batch_size=64, num_workers=0, n_classes=3, scale_factor=80,\n",
    "                                      wsi_type=\".svs\",sum_calc=True)\n",
    "    output_result = os.path.join(output_dir, str(t), 'output_result.csv')\n",
    "    data = []\n",
    "    pattern = re.compile(r\"(.+)_sum_out_label=(.+)\\.txt\")\n",
    "    for filename in os.listdir(os.path.join(output_dir,str(t))):\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            wsi_name, out_label = match.groups()\n",
    "            data.append([wsi_name, out_label])\n",
    "    df = pd.DataFrame(data, columns=[\"wsi_name\", \"out_label\"])\n",
    "    df.to_csv(output_result, index=False)\n",
    "    print(f\"转换完成，结果已保存至: {output_result}\")\n",
    "    print(df['out_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41041a14-195a-4f32-90da-0c47631b4b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid\n",
    "pick_index_df = pd.read_csv(r'.\\valid_txt\\pick_index.csv')\n",
    "output_dir = r'.\\valid_txt\\output'\n",
    "for t in range(1):\n",
    "    resnet_model,params_to_update = model_initialization(dimension=3,freeze=False)\n",
    "    model = model_load(resnet_model,device,\"stage2\\model_stage2_epochs_\"+str(t)+\".pth\")\n",
    "    model.eval()\n",
    "    make_preds_by_slide_and_visualize(model, pick_index_df, device, os.path.join(output_dir,str(t)), transform=data_transforms['valid'], \n",
    "                                      visualize=True,out_all=True,all_shuffle=False,batch_size=64, num_workers=0, n_classes=3, scale_factor=80,\n",
    "                                      wsi_type=\".mrxs\",sum_calc=True)\n",
    "    output_result = os.path.join(output_dir, str(t), 'output_result.csv')\n",
    "    data = []\n",
    "    pattern = re.compile(r\"(.+)_sum_out_label=(.+)\\.txt\")\n",
    "    for filename in os.listdir(os.path.join(output_dir,str(t))):\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            wsi_name, out_label = match.groups()\n",
    "            data.append([wsi_name, out_label])\n",
    "    df = pd.DataFrame(data, columns=[\"wsi_name\", \"out_label\"])\n",
    "    df.to_csv(output_result, index=False)\n",
    "    print(f\"转换完成，结果已保存至: {output_result}\")\n",
    "    print(df['out_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2c0975-dcd3-4332-9267-6517d53781b2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#E:\\THT\\HE_DTC_second\\second 里面还有一波mrxs的片子\n",
    "\n",
    "#将{wsi_name}_sum_out_label={out_lable}.txt转换为 output_result.csv\n",
    "output_dir = r'E:\\THT\\HE_DTC_TCGA\\output\\model2_out\\0'\n",
    "output_result = os.path.join(output_dir, 'output_result.csv')\n",
    "data = []\n",
    "pattern = re.compile(r\"(.+)_sum_out_label=(.+)\\.txt\")\n",
    "for filename in os.listdir(output_dir):\n",
    "    match = pattern.match(filename)\n",
    "    if match:\n",
    "        wsi_name, out_label = match.groups()\n",
    "        data.append([wsi_name, out_label])\n",
    "df = pd.DataFrame(data, columns=[\"wsi_name\", \"out_label\"])\n",
    "#df.to_csv(output_result, index=False)\n",
    "print(f\"转换完成，结果已保存至: {output_result}\")\n",
    "print(df['out_label'].value_counts())\n",
    "#转换完成，结果已保存至: E:\\THT\\HE_DTC_TCGA\\output\\model2_out\\18\\output_result.csv\n",
    "#out_label\n",
    "#0    467\n",
    "#2     20\n",
    "#Name: count, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9446a151-8a7c-43c3-937a-5d7b4140c6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime(\"%m月 %d日 %H时 %M分 %S秒\", time.gmtime(time.time())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
